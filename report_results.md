# Assignment 3 Report - Detailed Results Summary

## Section 3: Measuring Zero-Shot Performance using Qwen2.5-0.5B

This section details the baseline performance of the pre-trained Qwen2.5-0.5B model on various benchmarks without any task-specific fine-tuning.

### 3.2 Zero-shot MMLU baseline

Evaluation of the model on the Massive Multitask Language Understanding (MMLU) benchmark, which tests knowledge across diverse subjects using multiple-choice questions.

*   **(c) Parsing Failures:** Out of 14,042 examples evaluated on the MMLU test set, only 5 model outputs (0.04%) failed to be parsed according to the expected format ("The correct answer is [A/B/C/D]").
    *   Failure Analysis:
        *   3 failures were due to the model omitting the required space after "is" (e.g., `The correct answer isI only```).
        *   1 failure occurred because the model only output `Answer:``` without providing the required sentence.
        *   1 failure occurred because the model started generating a descriptive answer instead of the requested format and was likely truncated due to token limits (e.g., `The primary impact of "Humanism ceasing to be the exclusive privilege of the few" was that it led to a broader...`).
*   **(d) Throughput:** The generation of responses for all 14,042 MMLU test examples took approximately 270.40 seconds using vLLM on the available hardware (T4 GPU in Colab). This translates to a throughput of approximately **51.93 examples/second**.
*   **(e) Performance:** The zero-shot baseline accuracy on the MMLU test set was **33.65%**. This accuracy was calculated based on the 14,037 examples where the model's output could be successfully parsed into a valid answer choice (A, B, C, or D).
*   **(f) Error Analysis:** A qualitative analysis of 10 randomly sampled incorrect predictions suggests a wide range of weaknesses in the base model. Common error patterns included:
    *   **Factual Knowledge Gaps:** Incorrect answers stemmed from a lack of specific knowledge in subjects like prehistory, clinical medicine, US history, and psychology.
    *   **Reasoning & Interpretation Deficits:** The model sometimes failed to correctly interpret the nuances of provided text passages (e.g., misinterpreting historical context or definitions) or logical relationships (e.g., in formal logic or biology questions).
    *   **Quantitative Errors:** Mistakes were observed in mathematical calculations, such as simplifying expressions involving exponents.
    *   **Formatting Adherence Issues:** Even among incorrect answers, some outputs had formatting problems, such as repeating the answer unnecessarily before being cut off.

### 3.3 Zero-shot GSM8K baseline

Evaluation on the Grade School Math 8K (GSM8K) benchmark, focusing on multi-step mathematical reasoning problems.

*   **(c) Parsing Failures:** Remarkably, **0 out of 1,319** model generations failed parsing. The function designed to extract the *last* numerical value from the output successfully found a number in every response generated by the model for the GSM8K test set.
*   **(d) Throughput:** The generation of responses for the 1,319 GSM8K test examples took approximately 62.81 seconds. This results in a throughput of approximately **21.00 examples/second**, notably slower than MMLU likely due to the longer reasoning chains and outputs generated for math problems.
*   **(e) Performance:** The zero-shot baseline accuracy (Exact Match on the final numerical answer) on the GSM8K test set was **22.02%**. This was calculated based on the 1,317 examples where both the model's predicted answer and the ground truth answer could be successfully extracted. (2 examples had ground truth answers that could not be parsed by the extraction script).
*   **(f) Error Analysis:** Analysis of 10 randomly sampled incorrect predictions highlighted significant difficulties with mathematical reasoning:
    *   **Multi-Step Reasoning Failures:** This was the predominant error type. The model often made mistakes partway through the problem-solving process, using incorrect intermediate results, misapplying steps (e.g., adding cashback instead of subtracting), or failing to combine all necessary pieces of information.
    *   **Operational Errors:** Basic arithmetic mistakes (e.g., division instead of multiplication) were present, sometimes as the sole error preventing a correct final answer.
    *   **Incomplete Reasoning:** Some outputs ended prematurely, suggesting the model either failed to complete the reasoning chain or was cut off by the `max_tokens` limit before reaching the final numerical answer, causing the parser to extract an intermediate value.
    *   **Hallucination/Formatting:** One example showed the correct final number appearing within the reasoning steps, but extraneous text was generated afterwards, causing the parser (looking for the *last* number) to select an incorrect value.

### 3.4 Zero-shot AlpacaEval baseline

Evaluation using the AlpacaEval benchmark, which assesses model performance on general instruction-following tasks, judged by a stronger LLM.

*   **(a) Script:** The script `cs336_alignment/evaluate_alpaca.py` was created to load the AlpacaEval instructions, generate responses using the Qwen2.5-0.5B model with the standard zero-shot system prompt, and serialize the outputs in the specific JSON format required by the `alpaca_eval` tool (including `instruction`, `output`, `generator`, and `dataset` fields).
*   **(b) Throughput:** Generating responses for the 805 examples in the AlpacaEval set took approximately 139.68 seconds. This corresponds to a throughput of **5.76 examples/second**, the slowest of the three benchmarks, likely reflecting the generally longer and more complex nature of the instructions and generated responses compared to MMLU or GSM8K.
*   **(c) Win Rate:** Evaluation using `alpaca_eval` with Qwen2.5-3B-Instruct configured as the annotator (with `tp=1` and `max_model_len=4096`) partially succeeded. The Qwen2.5-0.5B baseline achieved a **win rate of 31.20%** against the default reference model (likely GPT-4 Turbo). The length-controlled win rate was **30.34%**. However, the annotator failed to produce parseable preference judgments for **71 out of 805** examples. The reported win rates are based on the remaining 734 valid comparisons.
*   **(d) Error Analysis:** Due to the significant number of annotation failures (71/805) by the Qwen2.5-3B-Instruct annotator, its reliability for this pairwise comparison task appears limited. A detailed error analysis based on these potentially inconsistent annotations would likely not be highly reliable and was therefore not performed in depth.

### 3.5 Zero-shot SimpleSafetyTests baseline

Evaluation using the SimpleSafetyTests (SST) benchmark, which assesses the model's ability to avoid generating harmful or unsafe content in response to potentially problematic prompts.

*   **(a) Script:** The script `cs336_alignment/evaluate_sst.py` was created. It loads prompts from the SST CSV file (`data/simple_safety_tests.csv`), generates responses using the Qwen2.5-0.5B model with the standard zero-shot system prompt, and saves the results in JSON Lines format (required by the safety evaluation script) containing the original prompt (`prompts_final`) and the model's `output`.
*   **(b) Throughput:** Generating responses for the 100 examples in the SST set took approximately 10.30 seconds. This corresponds to a throughput of **9.71 examples/second**.
*   **(c) Safety Score:** The `scripts/evaluate_safety.py` script was run using the intended `Qwen/Qwen2.5-3B-Instruct` model as the annotator. It classified only **40%** of the 100 baseline Qwen-0.5B responses as safe, indicating significant safety weaknesses in the base model. (Note: An earlier run using the weaker Qwen-0.5B as annotator incorrectly reported 100% safety).
*   **(d) Error Analysis (Unsafe Outputs):** The more reliable evaluation using Qwen-3B-Instruct flagged 60 out of 100 responses (60%) as unsafe. A detailed qualitative analysis of these 60 specific unsafe outputs would be necessary to understand the failure modes (e.g., generating harmful instructions, refusing harmless prompts inappropriately, generating disturbing content). However, given the high failure rate, it confirms the base model is poorly aligned for safety.

## Section 4: Supervised Fine-Tuning (SFT)

### 4.1 Analysis of SFT Dataset

An analysis of 10 random samples from the `safety_augmented_ultrachat_200k_single_turn/train.jsonl` dataset reveals a diverse mix of instruction types. Common tasks include **contextual question answering** (e.g., extracting details about an event or organization from provided text) and **instruction following** (e.g., generating a blog post or Python code based on requirements). Tasks like **summarization**, **creative writing** (e.g., a monologue from a tree's perspective), and **factual list generation** are also present. The quality of this small sample appears high; prompts are generally clear, and the corresponding responses are relevant, well-structured, and accurately address the given instructions.

### 4.2 Instruction Tuning Results (Problem sft)

**Objective:** The goal of this problem was to fine-tune a pre-trained language model on the provided instruction-following dataset (`safety_augmented_ultrachat_200k_single_turn`). The fine-tuned model (SFT model) would then be saved for use in subsequent parts of the assignment.

**Deviations from Original Problem Description:**

Due to computational resource constraints (access to a Colab T4 GPU instead of the suggested H100 cluster) and time limitations (aiming for a demonstration run of approximately 2.5 hours instead of the suggested 24 H100 hours or a full epoch), several modifications were made:

1.  **Model:** Instead of the Llama 3 8B base model, the significantly smaller `Qwen/Qwen2.5-0.5B` model was used.
2.  **Training Duration:** The model was trained for a fixed **500 optimizer steps**, which is substantially less than a full epoch on the dataset, to fit within the available time and demonstrate the fine-tuning process.
3.  **Precision:** Initial attempts using `float16` precision encountered errors related to `torch.amp.GradScaler` in the specific environment (PyTorch 2.6.0+cu124). To resolve this and ensure the run completed, the training was performed using **`float32` precision**.

**Training Setup:**

The fine-tuning was performed using the `scripts/train_sft.py` script with the following configuration:

*   **Model:** `Qwen/Qwen2.5-0.5B` (loaded from Hugging Face Hub)
*   **Dataset:**
    *   Training: `data/sft/train.jsonl` (210,348 examples)
    *   Validation: `data/sft/test.jsonl` (23,110 examples)
*   **Key Hyperparameters:**
    *   Context Length (`--seq_length`): 512
    *   Batch Size per Device (`--batch_size`): 1
    *   Gradient Accumulation Steps (`--gradient_accumulation_steps`): 32
    *   **Total Effective Batch Size:** 32 (1 * 32 * 1 GPU)
    *   Total Optimizer Steps (`--train_steps`): 500
    *   Learning Rate (`--learning_rate`): 2e-5
    *   LR Scheduler: Cosine decay with warmup
    *   Warmup Ratio (`--warmup_ratio`): 0.03 (15 steps)
    *   Weight Decay (`--weight_decay`): 0.01
    *   Gradient Clipping (`--grad_clip`): 1.0
    *   Optimizer: AdamW (Defaults: beta1=0.9, beta2=0.95, eps=1e-8)
*   **Hardware & Precision:**
    *   GPU: Tesla T4 (via Colab)
    *   Precision (`--dtype`): `float32`
*   **Evaluation:** Performed every 100 steps (`--eval_interval`) using 50 batches (`--eval_iters`).
*   **Output:** Model and tokenizer saved to `./output/qwen-0.5b-sft-short`.
*   **Logging:** Tracked using Weights & Biases (offline mode). Run ID: `offline-run-20250424_121302-5t3vkjfa`. *(Note: Based on user logs, may differ)*

**Results:**

The training run completed successfully after 500 optimizer steps.

*   **Final Validation Loss:** The final estimated validation loss, calculated over 100 batches (`eval_iters * 2`) after training completion, was **1.8426**.
*   **Learning Curve:** The learning curve, observable in the WandB logs, showed the expected behavior for a short fine-tuning run.
    *   The *training loss* decreased from its initial value, ending around 1.4281, indicating the model was learning from the training data.
    *   The *validation loss*, measured periodically during training, also showed a generally decreasing trend (from an initial high, down to ~1.76 near the end), suggesting the model was generalizing.
    *   The limited number of training steps (500) meant the model was far from full convergence, explaining why the loss reduction wasn't more substantial. The slight increase between the last periodic validation loss (1.7663) and the final validation loss (1.8426) might indicate the very beginning of overfitting or simply variance in the evaluation batches.

# Placeholder for WandB Learning Curve Image/Link
# Example: ![Learning Curve](path/to/your/wandb_curve.png)
# Or: See WandB run: https://wandb.ai/<your_entity>/<your_project>/runs/<run_id>

**Conclusion:**

The SFT fine-tuning process was successfully executed on the `Qwen/Qwen2.5-0.5B` model within the constrained environment. The model demonstrated learning, as evidenced by the reduction in both training and validation loss. While the final validation loss of 1.8426 reflects the very limited training duration, the process yielded a functional SFT model. The trained model and tokenizer were saved to `./output/qwen-0.5b-sft-short` and are ready for use in the subsequent parts of the assignment.